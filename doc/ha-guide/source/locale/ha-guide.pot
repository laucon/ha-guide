# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2015, OpenStack contributors
# This file is distributed under the same license as the High Availability Guide package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: High Availability Guide 0.0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2015-08-28 06:00+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../controller-ha-galera.rst:313
msgid "(Optional) Configure the :file:`clustercheck` utility."
msgstr ""

#: ../networking-ha-l3.rst:0
msgid "/etc/neutron/neutron.conf parameters for high availability"
msgstr ""

#: ../hardware-ha-basic.rst:25
msgid "10 GB"
msgstr ""

#: ../hardware-ha-basic.rst:21 ../hardware-ha-basic.rst:25
msgid "2 GB"
msgstr ""

#: ../networking-ha-l3.rst:29 ../networking-ha-l3.rst:32
msgid "2 or more"
msgstr ""

#: ../hardware-ha-basic.rst:21 ../hardware-ha-basic.rst:23
#: ../hardware-ha-basic.rst:25
msgid "3"
msgstr ""

#: ../hardware-ha-basic.rst:21 ../hardware-ha-basic.rst:23
msgid "5 GB"
msgstr ""

#: ../hardware-ha-basic.rst:23
msgid "512 MB"
msgstr ""

#: ../controller-ha-pacemaker.rst:416
msgid ":command:`# /etc/init.d/corosync start` (LSB)"
msgstr ""

#: ../controller-ha-pacemaker.rst:475
msgid ":command:`# /etc/init.d/pacemaker start` (LSB)"
msgstr ""

#: ../controller-ha-rabbitmq.rst:30
msgid ":command:`# apt-get install rabbitmq-server`"
msgstr ""

#: ../controller-ha-pacemaker.rst:418
msgid ":command:`# service corosync start` (LSB, alternate)"
msgstr ""

#: ../controller-ha-pacemaker.rst:477
msgid ":command:`# service pacemaker start` (LSB, alternate)"
msgstr ""

#: ../controller-ha-pacemaker.rst:420
msgid ":command:`# start corosync (upstart)`"
msgstr ""

#: ../controller-ha-pacemaker.rst:479
msgid ":command:`# start pacemaker` (upstart)"
msgstr ""

#: ../controller-ha-pacemaker.rst:422
msgid ":command:`# systemctl start corosync (systemd)`"
msgstr ""

#: ../controller-ha-pacemaker.rst:481
msgid ":command:`# systemctl start pacemaker` (systemd)"
msgstr ""

#: ../controller-ha-rabbitmq.rst:32
msgid ":command:`# yum install rabbitmq-server`"
msgstr ""

#: ../controller-ha-rabbitmq.rst:36
msgid ":command:`# zypper addrepo -f obs://Cloud:OpenStack:Kilo/SLE_12 Kilo`"
msgstr ""

#: ../controller-ha-rabbitmq.rst:34 ../controller-ha-rabbitmq.rst:40
msgid ":command:`# zypper install rabbitmq-server`"
msgstr ""

#: ../controller-ha-keystone.rst:55
msgid ""
":command:`crm configure` supports batch input so you may copy and paste the "
"above lines into your live Pacemaker configuration, and then make changes as "
"required. For example, you may enter edit ``p_ip_keystone`` from the :"
"command:`crm configure` menu and edit the resource to match your preferred "
"virtual IP address."
msgstr ""

#: ../storage-ha-cinder.rst:47
msgid ""
":command:`crm configure` supports batch input, so you may copy and paste the "
"lines above into your live pacemaker configuration and then make changes as "
"required. For example, you may enter ``edit p_ip_cinder-api`` from the :"
"command:`crm configure` menu and edit the resource to match your preferred "
"virtual IP address."
msgstr ""

#: ../controller-ha-rabbitmq.rst:12
msgid ""
":ref:`Configure OpenStack services to use Rabbit HA queues <rabbitmq-"
"services>`"
msgstr ""

#: ../controller-ha-rabbitmq.rst:10
msgid ":ref:`Configure RabbitMQ for HA queues<rabbitmq-configure>`"
msgstr ""

#: ../controller-ha-rabbitmq.rst:8
msgid ":ref:`Install RabbitMQ<rabbitmq-install>`"
msgstr ""

#: ../networking-ha.rst:35
msgid ":ref:`Neutron DHCP agent<dhcp-agent>`"
msgstr ""

#: ../networking-ha.rst:42
msgid ":ref:`Neutron L3 agent<neutron-l3>`"
msgstr ""

#: ../networking-ha.rst:44
msgid ":ref:`Neutron LBaaS<neutron-lbaas>` (Load Balancing as a Service) agent"
msgstr ""

#: ../networking-ha.rst:43
msgid ":ref:`Neutron metadata agent<neutron-metadata>`"
msgstr ""

#: ../controller-ha-pacemaker.rst:82
msgid ":ref:`corosync-multicast`"
msgstr ""

#: ../controller-ha-pacemaker.rst:83
msgid ":ref:`corosync-unicast`"
msgstr ""

#: ../controller-ha-pacemaker.rst:84
msgid ":ref:`corosync-votequorum`"
msgstr ""

#: ../storage-ha-glance.rst:11
msgid ":ref:`glance-api-configure`"
msgstr ""

#: ../storage-ha-glance.rst:10
msgid ":ref:`glance-api-pacemaker`"
msgstr ""

#: ../storage-ha-glance.rst:12
msgid ":ref:`glance-services`"
msgstr ""

#: ../storage-ha-cinder.rst:12
msgid ":ref:`ha-cinder-configure`"
msgstr ""

#: ../storage-ha-cinder.rst:11
msgid ":ref:`ha-cinder-pacemaker`"
msgstr ""

#: ../storage-ha-cinder.rst:13
msgid ":ref:`ha-cinder-services`"
msgstr ""

#: ../controller-ha-keystone.rst:17
msgid ":ref:`keystone-config-identity`"
msgstr ""

#: ../controller-ha-keystone.rst:16
msgid ":ref:`keystone-pacemaker`"
msgstr ""

#: ../controller-ha-keystone.rst:18
msgid ":ref:`keystone-services-config`"
msgstr ""

#: ../controller-ha-pacemaker.rst:42
msgid ":ref:`pacemaker-cluster-properties`"
msgstr ""

#: ../controller-ha-pacemaker.rst:39
msgid ":ref:`pacemaker-corosync-setup`"
msgstr ""

#: ../controller-ha-pacemaker.rst:40
msgid ":ref:`pacemaker-corosync-start`"
msgstr ""

#: ../controller-ha-pacemaker.rst:38
msgid ":ref:`pacemaker-install`"
msgstr ""

#: ../controller-ha-pacemaker.rst:41
msgid ":ref:`pacemaker-start`"
msgstr ""

#: ../index.rst:42
msgid ":ref:`search`"
msgstr ""

#: ../intro-ha-concepts.rst:138
msgid ":term:`active/active configuration`"
msgstr ""

#: ../intro-ha-concepts.rst:119
msgid ":term:`active/passive configuration`"
msgstr ""

#: ../intro-ha-concepts.rst:29
msgid ""
"A crucial aspect of high availability is the elimination of single points of "
"failure (SPOFs). A SPOF is an individual piece of equipment or software that "
"causes system downtime or data loss if it fails. In order to eliminate "
"SPOFs, check that mechanisms exist for redundancy of:"
msgstr ""

#: ../intro-ha-controller.rst:5
msgid ""
"A highly-available OpenStack environment must have a controller cluster with "
"three or more nodes. The following components are normally included in the "
"cluster."
msgstr ""

#: ../controller-ha-pacemaker.rst:353
msgid ""
"A sample votequorum service configuration in the :file:`corosync.com` file "
"is:"
msgstr ""

#: ../intro-ha-concepts.rst:81
msgid ""
"A service that provides a response after your request and then requires no "
"further attention. To make a stateless service highly available, you need to "
"provide redundant instances and load balance them. OpenStack services that "
"are stateless include ``nova-api``, ``nova-conductor``, ``glance-api``, "
"``keystone-api``, ``neutron-api`` and ``nova-scheduler``."
msgstr ""

#: ../intro-ha-concepts.rst:90
msgid ""
"A service where subsequent requests to the service depend on the results of "
"the first request. Stateful services are more difficult to manage because a "
"single action typically involves more than one request, so simply providing "
"additional instances and load balancing does not solve the problem. For "
"example, if the horizon user interface reset itself every time you went to a "
"new page, it would not be very useful. OpenStack services that are stateful "
"include the OpenStack database and message queue. Making stateful services "
"highly available can depend on whether you choose an active/passive or "
"active/active configuration."
msgstr ""

#: ../intro-ha-concepts.rst:132
msgid ""
"A typical active/active installation for a stateful service includes "
"redundant services, with all instances having an identical state. In other "
"words, updates to one instance of a database update all other instances. "
"This way a request to one instance is the same as a request to any other. A "
"load balancer manages the traffic to these systems, ensuring that "
"operational systems always handle the request."
msgstr ""

#: ../intro-ha-concepts.rst:114
msgid ""
"A typical active/passive installation for a stateful service maintains a "
"replacement resource that can be brought online when required. Requests are "
"handled using a :term:`virtual IP` address (VIP) that facilitates returning "
"to service with minimal reconfiguration. A separate application (such as "
"Pacemaker or Corosync) monitors these services, bringing the backup online "
"as necessary."
msgstr ""

#: ../intro-ha-controller.rst:81
msgid "AMQP (RabbitMQ)"
msgstr ""

#: ../hardware-ha-basic.rst:36
msgid ""
"Ability to take periodic \"snap shots\" throughout the installation process "
"and \"roll back\" to a working configuration in the event of a problem."
msgstr ""

#: ../index.rst:6
msgid "Abstract"
msgstr ""

#: ../intro-ha-concepts.rst:12
msgid "Accidental deletion or destruction of data."
msgstr ""

#: ../intro-ha-concepts.rst:103
msgid "Active/Passive vs Active/Active"
msgstr ""

#: ../storage-ha-cinder.rst:18
msgid "Add Block Storage API resource to Pacemaker"
msgstr ""

#: ../controller-ha-keystone.rst:23
msgid "Add OpenStack Identity resource to Pacemaker"
msgstr ""

#: ../storage-ha-glance.rst:22
msgid "Add OpenStack Image API resource to Pacemaker"
msgstr ""

#: ../controller-ha-galera.rst:80
msgid ""
"Adjust the configuration by making the following changes to the :file:`/etc/"
"mysql/my.cnf` file:"
msgstr ""

#: ../storage-ha-glance.rst:58
msgid ""
"After completing these steps, commit your configuration changes by entering :"
"command:`commit` from the :command:`crm configure` menu. Pacemaker then "
"starts the OpenStack Image API service and its dependent resources on one of "
"your nodes."
msgstr ""

#: ../controller-ha-pacemaker.rst:76
msgid ""
"After installing the Corosync package, you must create the :file:`/etc/"
"corosync/corosync.conf` configuration file. Corosync can be configured to "
"work with either multicast or unicast IP addresses or to use the votequorum "
"library."
msgstr ""

#: ../controller-ha-pacemaker.rst:471
msgid ""
"After the Corosync services have been started and you have verified that the "
"cluster is communicating properly, you can start :command:`pacemakerd`, the "
"Pacemaker master control process:"
msgstr ""

#: ../controller-ha-pacemaker.rst:483
msgid ""
"After the Pacemaker services have started, Pacemaker creates a default empty "
"cluster configuration with no resources. Use the :command:`crm_mon` utility "
"to observe the status of Pacemaker:"
msgstr ""

#: ../controller-ha-galera.rst:438
msgid ""
"After the output from the :command:`clustercheck` command is 200 on all "
"nodes, restart the MariaDB on node 1 with the following command sequence:"
msgstr ""

#: ../controller-ha-keystone.rst:63
msgid ""
"After you add these resources, commit your configuration changes by "
"entering :command:`commit` from the :command:`crm configure` menu. Pacemaker "
"then starts the OpenStack Identity service and its dependent resources on "
"one of your nodes."
msgstr ""

#: ../controller-ha-pacemaker.rst:559
msgid "After you make these changes, you may commit the updated configuration."
msgstr ""

#: ../controller-ha-pacemaker.rst:507
msgid ""
"After you set up your Pacemaker cluster, you should set a few basic cluster "
"properties using one of the following methods:"
msgstr ""

#: ../networking-ha-l3.rst:24
msgid "All routers are highly available by default."
msgstr ""

#: ../controller-ha-galera.rst:200
msgid ""
"Alternately, you can run the following command to print out just the "
"`password` line:"
msgstr ""

#: ../storage-ha-backend.rst:14
msgid "An OpenStack environment includes multiple data pools for the VMs:"
msgstr ""

#: ../intro-ha-concepts.rst:37
msgid "Applications and automatic service migration"
msgstr ""

#: ../controller-ha-galera.rst:158
msgid ""
"Be sure that SSH root access is established for the other database servers. "
"Then copy the :file:`debian.cnf` file to each other server and reset the "
"file permissions and owner to reduce the security risk. Do this by issuing "
"the following commands on the primary database server:"
msgstr ""

#: ../controller-ha-galera.rst:333
msgid "Be sure to supply a sensible password."
msgstr ""

#: ../storage-ha-backend.rst:27
msgid ""
"Block Storage service (cinder) can use LVM or Ceph RBD as the storage back "
"end."
msgstr ""

#: ../controller-ha-telemetry.rst:14
msgid ""
"Both the central and the compute agent can run in an HA deployment, which "
"means that multiple instances of these services can run in parallel with "
"workload partitioning among these running instances."
msgstr ""

#: ../install-ha-memcached.rst:38
msgid ""
"By default, `controller1` handles the caching service but, if the host goes "
"down, `controller2` does the job. For more information about memcached "
"installation, see the `OpenStack Cloud Administrator Guide <http://docs."
"openstack.org/admin-guide-cloud/>`_."
msgstr ""

#: ../install-ha-os.rst:12
msgid "CentOS, Fedora, RHEL: [link to RedHat install guide]"
msgstr ""

#: ../install-ha-ntp.rst:14
msgid "CentOS: [link to centos install guide]"
msgstr ""

#: ../storage-ha-backend.rst:56
msgid "Ceph"
msgstr ""

#: ../storage-ha-backend.rst:69
msgid ""
"Ceph RBD provides object replication capabilities by storing Block Storage "
"volumes as Ceph RBD objects; Ceph RBD ensures that each replica of an object "
"is stored on a different node. This means that your volumes are protected "
"against hard drive and node failures or even the failure of the data center "
"itself."
msgstr ""

#: ../controller-ha-pacemaker.rst:344
msgid "Change the number of expected votes for a cluster to be quorate"
msgstr ""

#: ../controller-ha-pacemaker.rst:342
msgid "Change the number of votes assigned to a node"
msgstr ""

#: ../intro-ha-concepts.rst:141
msgid "Clusters and quorums"
msgstr ""

#: ../controller-ha-rabbitmq.rst:28
msgid "Command"
msgstr ""

#: ../storage-ha-cinder.rst:62
msgid "Configure Block Storage API service"
msgstr ""

#: ../controller-ha-galera.rst:374
msgid "Configure MariaDB with Galera."
msgstr ""

#: ../install-ha-ntp.rst:4
msgid "Configure NTP"
msgstr ""

#: ../controller-ha-keystone.rst:72
msgid "Configure OpenStack Identity service"
msgstr ""

#: ../storage-ha-glance.rst:67
msgid "Configure OpenStack Image service API"
msgstr ""

#: ../controller-ha-rabbitmq.rst:174
msgid "Configure OpenStack services to use Rabbit HA queues"
msgstr ""

#: ../storage-ha-cinder.rst:88
msgid "Configure OpenStack services to use highly available Block Storage API"
msgstr ""

#: ../storage-ha-glance.rst:97
msgid ""
"Configure OpenStack services to use highly available OpenStack Image API"
msgstr ""

#: ../controller-ha-keystone.rst:104
msgid ""
"Configure OpenStack services to use the highly available OpenStack Identity"
msgstr ""

#: ../controller-ha-rabbitmq.rst:66
msgid "Configure RabbitMQ for HA queues"
msgstr ""

#: ../compute-node-ha-api.rst:4
msgid "Configure high availability on compute nodes"
msgstr ""

#: ../networking-ha.rst:6
msgid ""
"Configure networking on each node. The `Networking <http://docs.openstack."
"org/juno/install-guide/install/apt/content/ch_basic_environment.html#basics-"
"prerequisites>`_ section of the *Install Guide* includes basic information "
"about configuring networking."
msgstr ""

#: ../install-ha-vip.rst:4
msgid "Configure the VIP"
msgstr ""

#: ../controller-ha-galera.rst:147
msgid "Configure the database on other database servers"
msgstr ""

#: ../controller-ha-galera.rst:335
msgid ""
"Configure the monitor service (used by HAProxy) by creating the :file:`/etc/"
"xinetd.d/galera-monitor` file with the following contents:"
msgstr ""

#: ../intro-ha-storage.rst:8
msgid "Configuring Block Storage to listen on the VIP address"
msgstr ""

#: ../intro-ha-storage.rst:12
msgid "Configuring OpenStack services to use this IP address"
msgstr ""

#: ../storage-ha-backend.rst:50
msgid ""
"Configuring RAID on the hard drives that implement storage protects your "
"data against a hard drive failure. If, however, the node itself fails, data "
"may be lost. In particular, all volumes stored on an LVM node can be lost."
msgstr ""

#: ../storage-ha.rst:3
msgid "Configuring Storage for high availability"
msgstr ""

#: ../noncore-ha.rst:4
msgid "Configuring non-core components for high availability"
msgstr ""

#: ../compute-node-ha.rst:4
msgid "Configuring the compute node for high availability"
msgstr ""

#: ../controller-ha.rst:4
msgid "Configuring the controller for high availability"
msgstr ""

#: ../controller-ha-pacemaker.rst:346
msgid ""
"Connect an additional quorum device to allow small clusters remain quorate "
"during node outages"
msgstr ""

#: ../index.rst:21
msgid "Contents"
msgstr ""

#: ../controller-ha-galera.rst:111
msgid ""
"Copy this file to all other databases servers and change the value of "
"wsrep_cluster_address and wsrep_node_name accordingly."
msgstr ""

#: ../controller-ha-pacemaker.rst:247
msgid "Corosync configuration file fragment for unicast (corosync.conf)"
msgstr ""

#: ../controller-ha-pacemaker.rst:411
msgid ""
"Corosync is started as a regular system service. Depending on your "
"distribution, it may ship with an LSB init script, an upstart job, or a "
"systemd unit file. Either way, the service is usually named corosync:"
msgstr ""

#: ../controller-ha-galera.rst:376
msgid ""
"Create the :file:`/etc/my.cnf.d/galera.cnf` configuration file with the "
"following content:"
msgstr ""

#: ../controller-ha-galera.rst:91
msgid ""
"Create the :file:`/etc/mysql/conf.d/wsrep.cnf` file; paste the following "
"lines into this file:"
msgstr ""

#: ../controller-ha-galera.rst:321
msgid ""
"Create the :file:`etc/sysconfig/clustercheck` file with the following "
"contents:"
msgstr ""

#: ../controller-ha-galera.rst:358
msgid "Create the database user required by :command:`clustercheck`:"
msgstr ""

#: ../networking-ha-lbaas.rst:8
msgid ""
"Currently, no native feature is provided to make the LBaaS agent highly "
"available using the default plug-in HAProxy. A common way to make HAProxy "
"highly available is to use the VRRP (Virtual Router Redundancy Protocol). "
"Unfortunately, this is not yet implemented in the LBaaS HAProxy plug-in."
msgstr ""

#: ../intro-ha-concepts.rst:12
msgid "Data loss"
msgstr ""

#: ../controller-ha-galera.rst:4
msgid "Database (Galera/MySQL)"
msgstr ""

#: ../intro-ha-controller.rst:47
msgid "Database (MySQL/Galera)"
msgstr ""

#: ../networking-ha-l3.rst:21
msgid "Description"
msgstr ""

#: ../controller-ha-rabbitmq.rst:27
msgid "Distribution"
msgstr ""

#: ../controller-ha-galera.rst:305
msgid ""
"Distributions based on Red Hat include Galera packages in their "
"repositories. To install the most current version of the packages, run the "
"following command:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:179
msgid "Do this configuration on all services using RabbitMQ:"
msgstr ""

#: ../controller-ha-pacemaker.rst:200
msgid ""
"Each configured interface must have a unique ``ringnumber``, starting with 0."
msgstr ""

#: ../intro-ha-controller.rst:52
msgid ""
"Each instance has its own IP address; OpenStack services are configured with "
"the list of these IP addresses so they can select on of the addresses from "
"those available."
msgstr ""

#: ../intro-ha-concepts.rst:122
msgid ""
"Each service also has a backup but manages both the main and redundant "
"systems concurrently. This way, if there is a failure, the user is unlikely "
"to notice. The backup system is already online and takes on increased load "
"while the main system is fixed and brought back online."
msgstr ""

#: ../storage-ha-cinder.rst:64
msgid "Edit the :file:`/etc/cinder/cinder.conf` file:"
msgstr ""

#: ../storage-ha-glance.rst:69
msgid ""
"Edit the :file:`/etc/glance/glance-api.conf` file to configure the OpenStack "
"image service:"
msgstr ""

#: ../controller-ha-keystone.rst:75
msgid ""
"Edit the :file:`keystone.conf` file to change the values of the :manpage:"
"`bind(2)` parameters:"
msgstr ""

#: ../storage-ha-backend.rst:16
msgid ""
"Ephemeral storage is allocated for an instance and is deleted when the "
"instance is deleted. The Compute service manages ephemeral storage. By "
"default, Compute stores ephemeral drives as files on local disks on the "
"Compute node but Ceph RBD can instead be used as the storage back end for "
"ephemeral storage."
msgstr ""

#: ../controller-ha-pacemaker.rst:98
msgid "Example Corosync configuration file for multicast (corosync.conf)"
msgstr ""

#: ../install-ha-memcached.rst:32
msgid "Example configuration with two hosts:"
msgstr ""

#: ../intro-ha-concepts.rst:41
msgid "Facility services such as power, air conditioning, and fire protection"
msgstr ""

#: ../install-ha-ntp.rst:11
msgid "Follow the instructions in the appropriate Install Guide:"
msgstr ""

#: ../networking-ha.rst:19
msgid "For Kilo and beyond, focus on L3HA and DVR."
msgstr ""

#: ../controller-ha-keystone.rst:113
msgid ""
"For OpenStack Compute, for example, if your OpenStack Identiy service IP "
"address is 192.168.42.103, use the following configuration in your :file:"
"`api-paste.ini` file:"
msgstr ""

#: ../storage-ha-glance.rst:106
msgid ""
"For OpenStack Compute, for example, if your OpenStack Image API service IP "
"address is 192.168.42.103 (as in the configuration explained here), you "
"would use the following configuration in your :file:`nova.conf` file:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:45
msgid ""
"For SLES 12, the packages are signed by GPG key 893A90DAD85F9316. You should "
"verify the fingerprint of the imported GPG key before using it."
msgstr ""

#: ../controller-ha-pacemaker.rst:307
msgid ""
"For UDPU, every node that should be a member of the membership must be "
"specified."
msgstr ""

#: ../controller-ha-telemetry.rst:57
msgid ""
"For backward compatibility and supporting existing deployments, the central "
"agent configuration also supports using different configuration files for "
"groups of service instances of this type that are running in parallel. For "
"enabling this configuration, set a value for the partitioning_group_prefix "
"option in the `central section <http://docs.openstack.org/kilo/ config-"
"reference/content/ch_configuring-openstack-telemetry.html>`__ in the "
"OpenStack Configuration Reference."
msgstr ""

#: ../hardware-ha-basic.rst:29
msgid ""
"For demonstrations and studying, you can set up a test environment on "
"virtual machines (VMs). This has the following benefits:"
msgstr ""

#: ../controller-ha-haproxy.rst:15
msgid ""
"For detailed instructions about installing HAProxy on your nodes, see its "
"`official documentation <http://www.haproxy.org/#docs>`_. Note the following:"
msgstr ""

#: ../controller-ha-telemetry.rst:65
msgid ""
"For each sub-group of the central agent pool with the same "
"``partitioning_group_prefix`` a disjoint subset of meters must be polled -- "
"otherwise samples may be missing or duplicated. The list of meters to poll "
"can be set in the :file:`/etc/ceilometer/pipeline.yaml` configuration file. "
"For more information about pipelines see the `Data collection and processing "
"<http://docs.openstack.org/admin-guide-cloud/telemetry-data-collection."
"html#data-collection-and-processing>`__ section."
msgstr ""

#: ../controller-ha-pacemaker.rst:241
msgid ""
"For environments that do not support multicast, Corosync should be "
"configured for unicast. An example fragment of the :file:`corosync.conf` "
"file for unicastis shown below:"
msgstr ""

#: ../intro-ha-concepts.rst:157
msgid ""
"For example, in a 7-node cluster, the quorum could be set to 5 or 3. If "
"quorum is 5 and three nodes fail simultaneously, the cluster itself would "
"fail, whereas it would continue to function if the quorum were set to 3."
msgstr ""

#: ../controller-ha-pacemaker.rst:214
msgid ""
"For firewall configurations, note that Corosync communicates over UDP only, "
"and uses ``mcastport`` (for receives) and ``mcastport - 1`` (for sends)."
msgstr ""

#: ../controller-ha-telemetry.rst:39
msgid ""
"For information about the required configuration options that have to be set "
"in the :file:`ceilometer.conf` configuration file for both the central and "
"compute agents, see the `coordination section <http://docs.openstack.org/"
"kilo/config-reference/content/ ch_configuring-openstack-telemetry.html>`__ "
"in the OpenStack Configuration Reference."
msgstr ""

#: ../controller-ha-rabbitmq.rst:56
msgid ""
"For more information, see the official installation manual for the "
"distribution:"
msgstr ""

#: ../intro-ha-concepts.rst:151
msgid ""
"For this reason, each cluster in a high availability environment must have "
"an odd number of nodes and the quorum must specify an odd number of nodes. "
"If multiple nodes fail so that the cluster size falls below the quorum "
"value, the cluster itself fails."
msgstr ""

#: ../intro-ha-controller.rst:60
msgid ""
"Galera synchronous replication guarantees a zero slave lag. The failover "
"procedure completes once HAProxy detects that the active back end has gone "
"down and switches to the backup one, which is then marked as 'UP'. If no "
"back ends are up (in other words, the Galera cluster is not ready to accept "
"connections), the failover procedure finishes only when the Galera cluster "
"has been successfully reassembled. The SLA is normally no more than 5 "
"minutes."
msgstr ""

#: ../controller-ha-pacemaker.rst:338
msgid "Get a list of nodes known to the quorum service"
msgstr ""

#: ../intro-ha-controller.rst:33
msgid ""
"HAProxy is a load balancer that runs on each controller in the cluster but "
"does not synchronize the state. Each instance of HAProxy configures its "
"frontend to accept connections only from the Virtual IP (VIP) address and to "
"terminate them as a list of all instances of the corresponding service under "
"load balancing. For example, any OpenStack API service. This makes the "
"instances of HAProxy act independently and fail over transparently together "
"with the Network endpoints (VIP addresses) failover and shares the same SLA."
msgstr ""

#: ../controller-ha-haproxy.rst:4
msgid "HAProxy nodes"
msgstr ""

#: ../controller-ha-haproxy.rst:6
msgid ""
"HAProxy provides a fast and reliable HTTP reverse proxy and load balancer "
"for TCP and HTTP-based applications. It is particularly suited for web sites "
"crawling under very high loads while needing persistence or Layer 7 "
"processing. Supporting tens of thousands of connections is clearly realistic "
"with today’s hardware."
msgstr ""

#: ../controller-ha-haproxy.rst:19
msgid ""
"HAProxy should not be a single point of failure; you need to ensure its "
"availability by other means, such as Pacemaker or Keepalived."
msgstr ""

#: ../hardware-ha.rst:4
msgid "Hardware considerations for high availability"
msgstr ""

#: ../hardware-ha-basic.rst:4
msgid "Hardware setup"
msgstr ""

#: ../controller-ha-haproxy.rst:29
msgid ""
"Here is an example :file:`/etc/haproxy/haproxy.cfg` configuration file. "
"[TODO: Is a copy required on each controller node?] Note that you must "
"restart the HAProxy service to implement any changes made to this file."
msgstr ""

#: ../controller-ha-galera.rst:13
msgid ""
"High Availability for the OpenStack database can be achieved in many "
"different ways, depending on the type of database that is used in a "
"particular installation. Galera can be used with any of the following:"
msgstr ""

#: ../intro-ha-concepts.rst:3
msgid "High availability concepts"
msgstr ""

#: ../intro-ha-other.rst:4
msgid "High availability for other components"
msgstr ""

#: ../intro-ha-concepts.rst:23
msgid ""
"High availability is implemented with redundant hardware running redundant "
"instances of each service. If one piece of hardware running one instance of "
"a service fails, the system can then failover to use another instance of a "
"service that is running on hardware that did not fail."
msgstr ""

#: ../intro-ha-concepts.rst:5
msgid "High availability systems seek to minimize two things:"
msgstr ""

#: ../intro-ha-concepts.rst:52
msgid ""
"High availability systems typically achieve an uptime percentage of 99.99% "
"or more, which roughly equates to less than an hour of cumulative downtime "
"per year. In order to achieve this, high availability systems should keep "
"recovery times after a failure to about one to two minutes, sometimes "
"significantly less."
msgstr ""

#: ../storage-ha-cinder.rst:6
msgid "Highly available Block Storage API"
msgstr ""

#: ../storage-ha-glance.rst:3
msgid "Highly available OpenStack Image API"
msgstr ""

#: ../controller-ha-rabbitmq.rst:188
msgid ""
"How frequently to retry connecting with RabbitMQ: [TODO: document the unit "
"of measure here?  Seconds?]"
msgstr ""

#: ../controller-ha-rabbitmq.rst:195
msgid ""
"How long to back-off for between retries when connecting to RabbitMQ: [TODO: "
"document the unit of measure here?  Seconds?]"
msgstr ""

#: ../hardware-ha-basic.rst:11
msgid ""
"However, OpenStack does not require a significant amount of resources and "
"the following minimum requirements should support a proof-of-concept high "
"availability environment with core services and several instances:"
msgstr ""

#: ../hardware-ha-basic.rst:39
msgid ""
"However, running an OpenStack environment on VMs degrades the performance of "
"your instances, particularly if your hypervisor and/or processor lacks "
"support for hardware acceleration of nested VMs."
msgstr ""

#: ../controller-ha-keystone.rst:4
msgid "Identity services (keystone)"
msgstr ""

#: ../controller-ha-galera.rst:317
msgid ""
"If HAProxy is used to load-balance client access to MariaDB as described in "
"the HAProxy section of this document, you can use the :command:"
"`clustercheck` utility to improve health checks."
msgstr ""

#: ../controller-ha-pacemaker.rst:284
msgid ""
"If the ``broadcast`` parameter is set to yes, the broadcast address is used "
"for communication. If this option is set, the ``mcastaddr`` parameter should "
"not be set."
msgstr ""

#: ../controller-ha-rabbitmq.rst:153
msgid ""
"If the cluster is working, you can create usernames and passwords for the "
"queues."
msgstr ""

#: ../controller-ha-pacemaker.rst:225
msgid ""
"If you are using Corosync version 2 on Ubuntu 14.04, remove or comment out "
"lines under the service stanza, which enables Pacemaker to start up."
msgstr ""

#: ../controller-ha-pacemaker.rst:461
msgid ""
"If you are using Corosync version 2, use the :command:`corosync-cmapctl` "
"utility instead of :command:`corosync-objctl`; it is a direct replacement."
msgstr ""

#: ../controller-ha-keystone.rst:125
msgid ""
"If you are using both private and public IP addresses, you should create two "
"Virtual IP addresses and define your endpoint like this:"
msgstr ""

#: ../storage-ha-cinder.rst:98
msgid ""
"If you are using both private and public IP addresses, you should create two "
"Virtual IPs and define your endpoint like this:"
msgstr ""

#: ../controller-ha-keystone.rst:138
msgid ""
"If you are using the horizon dashboard, edit the :file:`local_settings.py` "
"file to include the following:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:222
msgid ""
"If you change the configuration from an old set-up that did not use HA "
"queues, you should restart the service:"
msgstr ""

#: ../controller-ha-galera.rst:77
msgid ""
"If you have already installed MariaDB, installing Galera will purge all "
"privileges; you must re-apply all the permissions listed in the installation "
"guide."
msgstr ""

#: ../storage-ha-backend.rst:29
msgid ""
"Image service (glance) can use the Object Storage service (swift) or Ceph "
"RBD as the storage back end."
msgstr ""

#: ../controller-ha-pacemaker.rst:193
msgid ""
"In Corosync configurations using redundant networking (with more than one "
"interface), you must select a Redundant Ring Protocol (RRP) mode other than "
"none. ``active`` is the recommended RRP mode."
msgstr ""

#: ../storage-ha-glance.rst:121
msgid ""
"In releases prior to Juno, this option was called ``glance_api_servers`` and "
"located in the [DEFAULT] section."
msgstr ""

#: ../intro-ha-concepts.rst:43
msgid ""
"In the event that a component fails and a back-up system must take on its "
"load, most high availability systems will replace the failed component as "
"quickly as possible to maintain necessary redundancy. This way time spent in "
"a degraded protection state is minimized."
msgstr ""

#: ../controller-ha-rabbitmq.rst:18 ../controller-ha-rabbitmq.rst:0
msgid "Install RabbitMQ"
msgstr ""

#: ../controller-ha-galera.rst:34
msgid ""
"Install a version of MySQL patched for wsrep (Write Set REPlication) from "
"`https://launchpad.net/codership-mysql`. The wsrep API supports synchronous "
"replication and so is suitable for configuring MySQL High Availability in "
"OpenStack."
msgstr ""

#: ../install-ha-memcached.rst:4
msgid "Install memcached"
msgstr ""

#: ../install-ha-os.rst:4
msgid "Install operating system on each node"
msgstr ""

#: ../controller-ha-pacemaker.rst:49
msgid "Install packages"
msgstr ""

#: ../controller-ha-galera.rst:32
msgid "Install the MySQL database on the primary database server"
msgstr ""

#: ../controller-ha-galera.rst:45
msgid ""
"Install the software properties, the key, and the repository; For Ubuntu "
"14.04 \"trusty\", the command sequence is:"
msgstr ""

#: ../install-ha.rst:3
msgid "Installing high availability packages"
msgstr ""

#: ../storage-ha-backend.rst:34
msgid ""
"Instructions for configuring storage back ends for the different storage "
"options is provided in:"
msgstr ""

#: ../intro-ha.rst:4
msgid "Introduction to OpenStack high availability"
msgstr ""

#: ../controller-ha-haproxy.rst:23
msgid ""
"It is advisable to have multiple HAProxy instances running, where the number "
"of these instances is a small odd number like 3 or 5."
msgstr ""

#: ../intro-ha-concepts.rst:178
msgid ""
"It is possible to add controllers to such an environment to convert it into "
"a truly highly available environment."
msgstr ""

#: ../networking-ha.rst:20
msgid ""
"Link to `Networking Guide <http://docs.openstack.org/networking-guide/>`_ "
"for configuration details."
msgstr ""

#: ../intro-ha-controller.rst:31
msgid "Load balancing (HAProxy)"
msgstr ""

#: ../intro-ha-concepts.rst:108
msgid ""
"Maintains a redundant instance that can be brought online when the active "
"service fails. For example, OpenStack writes to the main database while "
"maintaining a disaster recovery database that can be brought online if the "
"main database fails."
msgstr ""

#: ../intro-ha-storage.rst:5
msgid ""
"Making the Block Storage (cinder) API service highly available in active/"
"passive mode involves:"
msgstr ""

#: ../controller-ha-keystone.rst:13
msgid ""
"Making the OpenStack Identity service highly available in active / passive "
"mode involves:"
msgstr ""

#: ../storage-ha-cinder.rst:8
msgid ""
"Making the block storage (cinder) API service highly available in active/"
"passive mode involves:"
msgstr ""

#: ../intro-ha-storage.rst:10
msgid ""
"Managing the Block Storage API daemon with the Pacemaker cluster manager"
msgstr ""

#: ../controller-ha-galera.rst:291
msgid "MariaDB with Galera (Red Hat-based platforms)"
msgstr ""

#: ../controller-ha-galera.rst:293
msgid ""
"MariaDB with Galera provides synchronous database replication in an active-"
"active, multi-master environment. High availability for the data itself is "
"managed internally by Galera, while access availability is managed by "
"HAProxy."
msgstr ""

#: ../networking-ha-l3.rst:30
msgid "Maximum number of network nodes to use for the HA router."
msgstr ""

#: ../controller-ha-rabbitmq.rst:202
msgid ""
"Maximum retries with trying to connect to RabbitMQ (infinite by default):"
msgstr ""

#: ../intro-ha-controller.rst:103
msgid "Memcached back end"
msgstr ""

#: ../intro-ha-controller.rst:105
msgid ""
"Memcached is a memory cache demon that can be used by most OpenStack "
"services to store ephemeral data, such as tokens. Although Memcached does "
"not support typical forms of redundancy such as clustering, OpenStack "
"services can use almost any number of instances by configuring multiple "
"hostnames or IP addresses. The Memcached client implements hashing to "
"balance objects among the instances. Failure of an instance only impacts a "
"percentage of the objects and the client automatically removes it from the "
"list of instances. The SLA is several minutes."
msgstr ""

#: ../controller-ha-telemetry.rst:54
msgid ""
"Memcached uses a timeout value, which should always be set to a value that "
"is higher than the heartbeat value set for Telemetry."
msgstr ""

#: ../hardware-ha-basic.rst:19
msgid "Memory"
msgstr ""

#: ../install-ha-memcached.rst:26
msgid ""
"Memory caching is managed by `oslo.cache <http://specs.openstack.org/"
"openstack/oslo-specs/specs/kilo/oslo-cache-using-dogpile.html>`_ so the way "
"to use multiple memcached servers is the same for all projects."
msgstr ""

#: ../networking-ha-l3.rst:33
msgid ""
"Minimum number of network nodes to use for the HA router. A new router can "
"be created only if this number of network nodes are available."
msgstr ""

#: ../controller-ha-rabbitmq.rst:93
msgid ""
"Mirrored queues in RabbitMQ improve the availability of service since it is "
"resilient to failures."
msgstr ""

#: ../controller-ha-rabbitmq.rst:165
msgid "More information is available in the RabbitMQ documentation:"
msgstr ""

#: ../install-ha-memcached.rst:12
msgid ""
"Most OpenStack services can use memcached to store ephemeral data such as "
"tokens. Although memcached does not support typical forms of redundancy such "
"as clustering, OpenStack services can use almost any number of instances by "
"configuring multiple hostnames or IP addresses. The memcached client "
"implements hashing to balance objects among the instances. Failure of an "
"instance only impacts a percentage of the objects and the client "
"automatically removes it from the list of instances."
msgstr ""

#: ../controller-ha-pacemaker.rst:92
msgid ""
"Most distributions ship an example configuration file (:file:`corosync.conf."
"example`) as part of the documentation bundled with the Corosync package. An "
"example Corosync configuration file is shown below:"
msgstr ""

#: ../intro-ha-concepts.rst:48
msgid ""
"Most high availability systems fail in the event of multiple independent "
"(non-consequential) failures. In this case, most implementations favor "
"protecting data over maintaining availability."
msgstr ""

#: ../intro-ha-concepts.rst:14
msgid ""
"Most high availability systems guarantee protection against system downtime "
"and data loss only in the event of a single failure. However, they are also "
"expected to protect against cascading failures, where a single failure "
"deteriorates into a series of consequential failures."
msgstr ""

#: ../storage-ha-backend.rst:8
msgid ""
"Most of this guide concerns the control plane of high availability: ensuring "
"that services continue to run even if a component fails. Ensuring that data "
"is not lost is the data plane component of high availability; this is "
"discussed here."
msgstr ""

#: ../controller-ha-pacemaker.rst:206
msgid ""
"Multicast groups (``mcastaddr``) must not be reused across cluster "
"boundaries. In other words, no two distinct clusters should ever use the "
"same multicast group. Be sure to select multicast addresses compliant with "
"`RFC 2365, \"Administratively Scoped IP Multicast\" <http://www.ietf.org/rfc/"
"rfc2365.txt>`_."
msgstr ""

#: ../controller-ha-galera.rst:19
msgid ""
"MySQL is the most common choice; the next section tells how to configure "
"Galera/MySQL."
msgstr ""

#: ../intro-ha-controller.rst:49
msgid ""
"MySQL with Galera can be configured using one of the following strategies:"
msgstr ""

#: ../intro-ha-controller.rst:15
msgid "Network components"
msgstr ""

#: ../intro-ha-concepts.rst:35
msgid "Network components, such as switches and routers"
msgstr ""

#: ../networking-ha.rst:36
msgid ""
"Neutron L2 agent. Note that the L2 agent cannot be distributed and highly "
"available. Instead, it must be installed on each data forwarding node to "
"control the virtual network drivers such as Open vSwitch or Linux Bridge. "
"One L2 agent runs per node and controls its virtual interfaces."
msgstr ""

#: ../networking-ha.rst:17
msgid ""
"Neutron agents shuld be described for active/active; deprecate single "
"agent's instances case."
msgstr ""

#: ../controller-ha-galera.rst:149
msgid ""
"Next, you need to copy the database configuration to the other database "
"servers. Before doing this, make a backup copy of this file that you can use "
"to recover from an error:"
msgstr ""

#: ../networking-ha-metadata.rst:8
msgid ""
"No native feature is available to make this service highly available. At "
"this time, the Active/Passive solution exists to run the neutron metadata "
"agent in failover mode with Pacemaker."
msgstr ""

#: ../hardware-ha-basic.rst:19
msgid "Node type"
msgstr ""

#: ../controller-ha-pacemaker.rst:198
msgid "Note the following about the recommended interface configuration:"
msgstr ""

# #-#-#-#-#  controller-ha-haproxy.pot (High Availability Guide 0.0.1)  #-#-#-#-#
# #-#-#-#-#  controller-ha-pacemaker.pot (High Availability Guide 0.0.1)  #-#-#-#-#
#: ../controller-ha-haproxy.rst:190 ../controller-ha-pacemaker.rst:169
#: ../controller-ha-pacemaker.rst:282 ../controller-ha-pacemaker.rst:366
#: ../controller-ha-pacemaker.rst:526
msgid "Note the following:"
msgstr ""

#: ../networking-ha.rst:11
msgid "Notes from planning outline:"
msgstr ""

#: ../controller-ha-galera.rst:206
msgid ""
"Now run the following query on each server other than the primary database "
"node. This will ensure that you can restart the database again. You will "
"need to supply the password you got in the previous step:"
msgstr ""

#: ../intro-ha-concepts.rst:8
msgid ""
"Occurs when a user-facing service is unavailable beyond a specified maximum "
"amount of time."
msgstr ""

#: ../controller-ha-pacemaker.rst:51
msgid ""
"On any host that is meant to be part of a Pacemaker cluster, you must first "
"establish cluster communications through the Corosync messaging layer. This "
"involves installing the following packages (and their dependencies, which "
"your package manager usually installs automatically):"
msgstr ""

#: ../controller-ha-rabbitmq.rst:113
msgid ""
"On each target node, verify the correct owner, group, and permissions of the "
"file :file:`erlang.cookie`."
msgstr ""

#: ../controller-ha-galera.rst:426
msgid "On node 1, run the following command:"
msgstr ""

#: ../controller-ha-galera.rst:432
msgid "On nodes 2 and 3, run the following command:"
msgstr ""

#: ../storage-ha-cinder.rst:54
msgid ""
"Once completed, commit your configuration changes by entering :command:"
"`commit` from the :command:`crm configure` menu. Pacemaker then starts the "
"Block Storage API service and its dependent resources on one of your nodes."
msgstr ""

#: ../controller-ha-pacemaker.rst:231
msgid ""
"Once created, the :file:`corosync.conf` file (and the :file:`authkey` file "
"if the secauth option is enabled) must be synchronized across all cluster "
"nodes."
msgstr ""

#: ../hardware-ha-basic.rst:33
msgid ""
"One physical server can support multiple nodes, each of which supports "
"almost any number of network interfaces."
msgstr ""

#: ../controller-ha-galera.rst:407
msgid "Open the firewall ports used for MariaDB and Galera communications:"
msgstr ""

#: ../index.rst:3
msgid "OpenStack High Availability Guide"
msgstr ""

#: ../controller-ha-keystone.rst:6
msgid ""
"OpenStack Identity (keystone) is the Identity Service in OpenStack that is "
"used by many services. You should be familiar with `OpenStack identity "
"concepts <http://docs.openstack.org/juno/install-guide/install/apt/content/"
"keystone-concepts.html>`_ before proceeding."
msgstr ""

#: ../controller-ha-rabbitmq.rst:83
msgid "OpenStack block storage"
msgstr ""

#: ../controller-ha-rabbitmq.rst:82
msgid "OpenStack compute"
msgstr ""

#: ../intro-ha-concepts.rst:58
msgid ""
"OpenStack currently meets such availability requirements for its own "
"infrastructure services, meaning that an uptime of 99.99% is feasible for "
"the OpenStack infrastructure proper. However, OpenStack does not guarantee "
"99.99% availability for individual guest instances."
msgstr ""

#: ../controller-ha-pacemaker.rst:6
msgid ""
"OpenStack infrastructure high availability relies on the `Pacemaker <http://"
"clusterlabs.org/>`_ cluster stack, the state-of-the-art high availability "
"and load balancing stack for the Linux platform. Pacemaker is storage and "
"application-agnostic, and is in no way specific to OpenStack."
msgstr ""

#: ../networking-ha.rst:4
msgid "OpenStack network nodes"
msgstr ""

#: ../networking-ha.rst:33
msgid "OpenStack network nodes contain:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:84
msgid "OpenStack networking"
msgstr ""

#: ../intro-ha-concepts.rst:171
msgid ""
"OpenStack supports a single-controller high availability mode that is "
"managed by the services that manage highly available environments but is not "
"actually highly available because no redundant controllers are configured to "
"use for failover. This environment can be used for study and demonstration "
"but is not appropriate for a production environment."
msgstr ""

#: ../intro-ha-storage.rst:3
msgid "Overview of high availability storage"
msgstr ""

#: ../intro-ha-compute.rst:4
msgid "Overview of highly-available compute nodes"
msgstr ""

#: ../intro-ha-controller.rst:3
msgid "Overview of highly-available controllers"
msgstr ""

#: ../controller-ha-pacemaker.rst:4
msgid "Pacemaker cluster stack"
msgstr ""

#: ../controller-ha-pacemaker.rst:20
msgid ""
"Pacemaker does not inherently (need or want to) understand the applications "
"it manages. Instead, it relies on resource agents (RAs), scripts that "
"encapsulate the knowledge of how to start, stop, and check the health of "
"each application managed by the cluster."
msgstr ""

#: ../controller-ha-pacemaker.rst:13
msgid ""
"Pacemaker relies on the `Corosync <http://corosync.github.io/corosync/>`_ "
"messaging layer for reliable cluster communications. Corosync implements the "
"Totem single-ring ordering and membership protocol. It also provides UDP and "
"InfiniBand based messaging, quorum, and cluster membership to Pacemaker."
msgstr ""

#: ../controller-ha-pacemaker.rst:30
msgid ""
"Pacemaker ships with a large set of OCF agents (such as those managing MySQL "
"databases, virtual IP addresses, and RabbitMQ), but can also use any agents "
"already installed on your system and can be extended with your own (see the "
"`developer guide <http://www.linux-ha.org/doc/dev-guides/ra-dev-guide."
"html>`_)."
msgstr ""

#: ../controller-ha-pacemaker.rst:553
msgid ""
"Pacemaker uses an event-driven approach to cluster state processing. The "
"``cluster-recheck-interval`` parameter (which defaults to 15 minutes) "
"defines the interval at which certain Pacemaker actions occur. It is usually "
"prudent to reduce this to a shorter interval, such as 5 or 3 minutes."
msgstr ""

#: ../networking-ha-l3.rst:19
msgid "Parameter"
msgstr ""

#: ../storage-ha-backend.rst:24
msgid ""
"Persistent storage exists outside all instances. Two types of persistent "
"storage are provided:"
msgstr ""

#: ../controller-ha-pacemaker.rst:310
msgid "Possible options are:"
msgstr ""

#: ../intro-ha-concepts.rst:77
msgid ""
"Preventing single points of failure can depend on whether or not a service "
"is stateless."
msgstr ""

#: ../hardware-ha-basic.rst:19
msgid "Processor"
msgstr ""

#: ../controller-ha-pacemaker.rst:531
msgid ""
"Production environments should not set the `` no-quorum-policy=\"ignore\"`` "
"parameter."
msgstr ""

#: ../controller-ha-rabbitmq.rst:96
msgid ""
"Production servers should run (at least) three RabbitMQ servers; for testing "
"and demonstration purposes, it is possible to run only two servers. In this "
"section, we configure two nodes, called `rabbit1` and `rabbit2`. To build a "
"broker, we need to ensure that all nodes have the same Erlang cookie file."
msgstr ""

#: ../controller-ha-pacemaker.rst:336
msgid "Query the quorum status"
msgstr ""

#: ../storage-ha-backend.rst:48
msgid "RAID drives"
msgstr ""

#: ../controller-ha-rabbitmq.rst:31
msgid "RHEL, Fedora, CentOS"
msgstr ""

#: ../controller-ha-rabbitmq.rst:3
msgid "RabbitMQ"
msgstr ""

#: ../controller-ha-rabbitmq.rst:181
msgid "RabbitMQ HA cluster host:port pairs: [TODO: Add rabbit3 if you agree]"
msgstr ""

#: ../controller-ha-rabbitmq.rst:5
msgid ""
"RabbitMQ is the default AMQP server used by many OpenStack services. Making "
"the RabbitMQ service highly available involves the following steps:"
msgstr ""

#: ../intro-ha-controller.rst:83
msgid ""
"RabbitMQ nodes fail over both on the application and the infrastructure "
"layers. The application layer is controlled by the ``oslo.messaging`` "
"configuration options for multiple AMQP hosts. If the AMQP node fails, the "
"application reconnects to the next one configured within the specified "
"reconnect interval. The specified reconnect interval constitutes its SLA. On "
"the infrastructure layer, the SLA is the time for which RabbitMQ cluster "
"reassembles. Several cases are possible. The Mnesia keeper node is the "
"master of the corresponding Pacemaker resource for RabbitMQ; when it fails, "
"the result is a full AMQP cluster downtime interval. Normally, its SLA is no "
"more than several minutes. Failure of another node that is a slave of the "
"corresponding Pacemaker resource for RabbitMQ results in no AMQP cluster "
"downtime at all."
msgstr ""

#: ../networking-ha.rst:13
msgid ""
"Rather than configuring neutron here, we should simply mention physical "
"network HA methods such as bonding and additional node/network requirements "
"for L3HA and DVR for planning purposes."
msgstr ""

#: ../controller-ha-pacemaker.rst:340
msgid "Receive notifications of quorum state changes"
msgstr ""

#: ../controller-ha-telemetry.rst:34
msgid "Recommended for testing."
msgstr ""

#: ../controller-ha-telemetry.rst:28 ../controller-ha-telemetry.rst:31
msgid "Recommended solution by the Tooz project."
msgstr ""

#: ../storage-ha-backend.rst:41
msgid ""
"Red Hat Linux Enterprise, CentOS, and Fedora: [link to Red Hat install guide]"
msgstr ""

#: ../install-ha-ntp.rst:15
msgid "RedHat: [link to redhat install guide]"
msgstr ""

#: ../intro-ha-concepts.rst:21
msgid "Redundancy and failover"
msgstr ""

#: ../storage-ha-backend.rst:85
msgid "Remote backup facilities"
msgstr ""

#: ../controller-ha-galera.rst:120
msgid "Remove user accounts with empty user names because they cause problems:"
msgstr ""

#: ../controller-ha-galera.rst:105
msgid ""
"Replace (PRIMARY_NODE_IP}, {SECONDARY_NODE}, and (TERTIARY__NODE_IP} with "
"the IP addresses of your servers."
msgstr ""

#: ../controller-ha-galera.rst:108
msgid ""
"Replace {NODE_NAME} with the hostname of the server. This is set for logging."
msgstr ""

#: ../networking-ha-dhcp.rst:6
msgid "Run neutron DHCP agent"
msgstr ""

#: ../networking-ha-l3.rst:6
msgid "Run neutron L3 agent"
msgstr ""

#: ../networking-ha-lbaas.rst:6
msgid "Run neutron LBaaS agent"
msgstr ""

#: ../networking-ha-metadata.rst:6
msgid "Run neutron metadata agent"
msgstr ""

#: ../controller-ha-rabbitmq.rst:132
msgid "Run the following commands on each node except the first one:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:35
msgid "SLES 12"
msgstr ""

#: ../index.rst:40
msgid "Search in this guide"
msgstr ""

#: ../intro-ha-controller.rst:44
msgid "See [TODO link] for information about configuring HAProxy."
msgstr ""

#: ../intro-ha-controller.rst:27
msgid ""
"See [TODO link] for more information about configuring networking for high "
"availability."
msgstr ""

#: ../networking-ha-l3.rst:27
msgid "Set automatic L3 agent failover for routers"
msgstr ""

#: ../controller-ha-pacemaker.rst:505
msgid "Set basic cluster properties"
msgstr ""

#: ../controller-ha-pacemaker.rst:516
msgid "Set the following properties:"
msgstr ""

#: ../controller-ha-pacemaker.rst:74
msgid "Set up Corosync"
msgstr ""

#: ../controller-ha-pacemaker.rst:90
msgid "Set up Corosync with multicast"
msgstr ""

#: ../controller-ha-pacemaker.rst:239
msgid "Set up Corosync with unicast"
msgstr ""

#: ../controller-ha-pacemaker.rst:328
msgid "Set up Corosync with votequorum library"
msgstr ""

#: ../controller-ha-pacemaker.rst:382
msgid ""
"Setting ``last_man_standing`` to 1 enables the Last Man Standing (LMS) "
"feature; by default, it is disabled (set to 0). If a cluster is on the "
"quorum edge (``expected_votes:``set to 7; ``online nodes:`` set to 4) for "
"longer than the time specified for the ``last_man_standing_window`` "
"parameter, the cluster can recalculate quorum and continue operating even if "
"the next node will be lost. This logic is repeated until the number of "
"online nodes in the cluster reaches 2. In order to allow the cluster to step "
"down from 2 members to only 1, the ``auto_tie_breaker`` parameter needs to "
"be set; this is not recommended for production environments."
msgstr ""

#: ../controller-ha-pacemaker.rst:376
msgid ""
"Setting ``wait_for_all`` to 1 means that, When starting up a cluster (all "
"nodes down), the cluster quorum is held until all nodes are online and have "
"joined the cluster for the first time. This parameter is new in Corosync 2.0."
msgstr ""

#: ../controller-ha-pacemaker.rst:547
msgid ""
"Setting the ``pe-warn-series-max``, ``pe-input-series-max`` and ``pe-error-"
"series-max`` parameters to 1000 instructs Pacemaker to keep a longer history "
"of the inputs processed and errors and warnings generated by its Policy "
"Engine. This history is useful if you need to troubleshoot the cluster."
msgstr ""

#: ../intro-ha-concepts.rst:169
msgid "Single-controller high availability mode"
msgstr ""

#: ../controller-ha-pacemaker.rst:368
msgid ""
"Specifying ``corosync_votequorum`` enables the votequorum library; this is "
"the only required option."
msgstr ""

#: ../controller-ha-galera.rst:114
msgid "Start :command:`mysql` as root and execute the following queries:"
msgstr ""

#: ../controller-ha-pacemaker.rst:409
msgid "Start Corosync"
msgstr ""

#: ../controller-ha-pacemaker.rst:469
msgid "Start Pacemaker"
msgstr ""

#: ../controller-ha-rabbitmq.rst:121
msgid "Start RabbitMQ on all nodes and verify that the nodes are running:"
msgstr ""

#: ../controller-ha-galera.rst:222
msgid "Start all the other nodes with the following command:"
msgstr ""

#: ../controller-ha-pacemaker.rst:511
msgid ""
"Start the :command:`crm` shell and enter :command:`configure` to change into "
"the configuration menu."
msgstr ""

#: ../controller-ha-galera.rst:366
msgid "Start the :command:`xinetd` daemon required by :command:`clustercheck`:"
msgstr ""

#: ../controller-ha-galera.rst:424
msgid "Start the MariaDB cluster:"
msgstr ""

#: ../intro-ha-concepts.rst:100
msgid "Stateful service"
msgstr ""

#: ../intro-ha-concepts.rst:105
msgid "Stateful services may be configured as active/passive or active/active:"
msgstr ""

#: ../intro-ha-concepts.rst:87
msgid "Stateless service"
msgstr ""

#: ../intro-ha-concepts.rst:75
msgid "Stateless vs. stateful services"
msgstr ""

#: ../controller-ha-galera.rst:215
msgid ""
"Stop all the mysql servers and start the first server with the following "
"command:"
msgstr ""

#: ../hardware-ha-basic.rst:19
msgid "Storage"
msgstr ""

#: ../storage-ha-backend.rst:6
msgid "Storage back end"
msgstr ""

#: ../intro-ha-concepts.rst:39
msgid "Storage components"
msgstr ""

#: ../install-ha-ntp.rst:16
msgid "SuSe: [link to suse install guide]"
msgstr ""

#: ../intro-ha-concepts.rst:9
msgid "System downtime"
msgstr ""

# #-#-#-#-#  controller-ha-rabbitmq.pot (High Availability Guide 0.0.1)  #-#-#-#-#
# #-#-#-#-#  controller-ha-telemetry.pot (High Availability Guide 0.0.1)  #-#-#-#-#
#: ../controller-ha-rabbitmq.rst:85 ../controller-ha-telemetry.rst:4
msgid "Telemetry"
msgstr ""

#: ../controller-ha-telemetry.rst:9
msgid "Telemetry central agent"
msgstr ""

#: ../storage-ha-glance.rst:51
msgid ""
"The :command:`crm configure` command  supports batch input, so you may copy "
"and paste the above into your live Pacemaker configuration and then make "
"changes as required. For example, you may enter edit ``p_ip_glance-api`` "
"from the :command:`crm configure` menu and edit the resource to match your "
"preferred virtual IP address."
msgstr ""

#: ../controller-ha-haproxy.rst:192
msgid ""
"The Galera cluster configuration commands indicate that two of the three "
"controllers are standby nodes. [TODO: be specific about the coding that "
"defines this] This ensures that only one node services write requests "
"because OpenStack support for multi-node writes is not yet production-ready."
msgstr ""

#: ../intro-ha-controller.rst:56
msgid ""
"The MySQL/Galera cluster runs behind HAProxy. HAProxy the load balances "
"incoming requests and exposes just one IP address for all the clients."
msgstr ""

#: ../storage-ha-glance.rst:5
msgid ""
"The OpenStack Image service offers a service for discovering, registering, "
"and retrieving virtual machine images. To make the OpenStack Image API "
"service highly available in active / passive mode, you must:"
msgstr ""

#: ../install-ha-os.rst:16
msgid ""
"The OpenStack Installation Guides also include a list of the services that "
"use passwords with important notes about using them."
msgstr ""

#: ../networking-ha-dhcp.rst:8
msgid ""
"The OpenStack Networking service has a scheduler that lets you run multiple "
"agents across nodes; the DHCP agent can be natively highly available. To "
"configure the number of DHCP agents per network, modify the "
"`dhcp_agents_per_network` parameter in the :file:`/etc/neutron/neutron.conf` "
"file. By default this is set to 1. To achieve high availability, assign more "
"than one DHCP agent per network."
msgstr ""

#: ../controller-ha-telemetry.rst:11
msgid ""
"The Telemetry central agent can be configured to partition its polling "
"workload between multiple agents, enabling high availability."
msgstr ""

#: ../compute-node-ha-api.rst:6
msgid ""
"The `Installation Guide <http://docs.openstack.org/juno/install-guide/"
"install/apt/content/ch_nova.html>` gives instructions for installing "
"multiple compute nodes. To make them highly available, you must configure "
"the envirionment to include multiple instances of the API and other services."
msgstr ""

#: ../controller-ha-telemetry.rst:18
msgid ""
"The `Tooz <https://pypi.python.org/pypi/tooz>`__ library provides the "
"coordination within the groups of service instances. It provides an API "
"above several back ends that can be used for building distributed "
"applications."
msgstr ""

#: ../controller-ha-keystone.rst:84
msgid ""
"The ``admin_bind_host`` parameter lets you use a private network for admin "
"access."
msgstr ""

#: ../controller-ha-pacemaker.rst:203
msgid ""
"The ``bindnetaddr`` is the network address of the interfaces to bind to. The "
"example uses two network addresses of /24 IPv4 subnets."
msgstr ""

#: ../controller-ha-pacemaker.rst:171
msgid ""
"The ``token`` value specifies the time, in milliseconds, during which the "
"Corosync token is expected to be transmitted around the ring. When this "
"timeout expires, the token is declared lost, and after "
"``token_retransmits_before_loss_const lost`` tokens, the non-responding "
"processor (cluster node) is declared dead. In other words, ``token × "
"token_retransmits_before_loss_const`` is the maximum time a node is allowed "
"to not respond to cluster messages before being considered dead. The default "
"for token is 1000 milliseconds (1 second), with 4 allowed retransmits. These "
"defaults are intended to minimize failover times, but can cause frequent "
"\"false alarms\" and unintended failovers in case of short network "
"interruptions. The values used here are safer, albeit with slightly extended "
"failover times."
msgstr ""

#: ../controller-ha-pacemaker.rst:288
msgid ""
"The ``transport`` directive controls the transport mechanism used. To avoid "
"the use of multicast entirely, specify the ``udpu`` unicast transport "
"parameter. This requires specifying the list of members in the ``nodelist`` "
"directive; this could potentially make up the membership before deployment. "
"The default is ``udp``. The transport type can also be set to ``udpu`` or "
"``iba``."
msgstr ""

#: ../controller-ha-telemetry.rst:50
msgid ""
"The availability check of the instances is provided by heartbeat messages. "
"When the connection with an instance is lost, the workload will be "
"reassigned within the remained instances in the next polling cycle."
msgstr ""

#: ../controller-ha.rst:6
msgid ""
"The cloud controller runs on the management network and must talk to all "
"other services."
msgstr ""

#: ../controller-ha-pacemaker.rst:371
msgid ""
"The cluster is fully operational with ``expected_votes`` set to 7 nodes "
"(each node has 1 vote), quorum: 4. If a list of nodes is specified as "
"``nodelist``, the ``expected_votes`` value is ignored."
msgstr ""

#: ../controller-ha-rabbitmq.rst:20
msgid ""
"The commands for installing RabbitMQ are specific to the Linux distribution "
"you are using:"
msgstr ""

#: ../controller-ha-haproxy.rst:26
msgid ""
"The common practice is to locate an HAProxy instance on each OpenStack "
"controller in the environment."
msgstr ""

#: ../intro-ha-controller.rst:20
msgid ""
"The configuration uses static routing without Virtual Router Redundancy "
"Protocol (VRRP) or similar techniques implemented."
msgstr ""

#: ../install-ha-os.rst:6
msgid ""
"The first step in setting up your highly-available OpenStack cluster is to "
"install the operating system on each node. Follow the instructions in the "
"OpenStack Installation Guides:"
msgstr ""

#: ../controller-ha-galera.rst:6
msgid ""
"The first step is installing the database that sits at the heart of the "
"cluster. To implement High Availability, run an instance of the database on "
"each controller node, using Galera for synchronous multi-master replication. "
"The Galera Cluster plug-in is a multi-master cluster based on synchronous "
"replication. It is a high availability service that provides high system "
"uptime, no data loss, and scalability for growth."
msgstr ""

#: ../controller-ha-rabbitmq.rst:74
msgid "The following components/services can work with HA queues:"
msgstr ""

#: ../controller-ha-galera.rst:70
msgid ""
"The galara package is now called galera-3 and is already a dependency of "
"mariadb-galera-server. Therefore it should not be specified on the command "
"line."
msgstr ""

#: ../networking-ha-l3.rst:8
msgid ""
"The neutron L3 agent is scalable, due to the scheduler that supports Virtual "
"Router Redundancy Protocol (VRRP) to distribute virtual routers across "
"multiple nodes. To enable high availability for configured routers, edit "
"the :file:`/etc/neutron/neutron.conf` file to set the following values:"
msgstr ""

#: ../intro-ha-concepts.rst:143
msgid ""
"The quorum specifies the minimal number of nodes that must be functional in "
"a cluster of redundant nodes in order for the cluster to remain functional. "
"When one node fails and failover transfers control to other nodes, the "
"system must ensure that data and processes remain sane. To determine this, "
"the contents of the remaining nodes are compared and, if there are "
"discrepancies, a \"majority rules\" algorithm is implemented."
msgstr ""

#: ../controller-ha-galera.rst:184
msgid "The result will be similar to this:"
msgstr ""

#: ../controller-ha-pacemaker.rst:219
msgid ""
"The service declaration for the pacemaker service may be placed in the :file:"
"`corosync.conf` file directly or in its own separate file, :file:`/etc/"
"corosync/service.d/pacemaker`."
msgstr ""

#: ../hardware-ha-basic.rst:6
msgid "The standard hardware requirements:"
msgstr ""

#: ../controller-ha-pacemaker.rst:36
msgid "The steps to implement the Pacemaker cluster stack are:"
msgstr ""

#: ../controller-ha-pacemaker.rst:349
msgid ""
"The votequorum library has been created to replace and eliminate qdisk, the "
"disk-based quorum daemon for CMAN, from advanced cluster configurations."
msgstr ""

#: ../controller-ha-pacemaker.rst:330
msgid ""
"The votequorum library is part of the corosync project. It provides an "
"interface to the vote-based quorum service and it must be explicitly enabled "
"in the Corosync configuration file. The main role of votequorum library is "
"to avoid split-brain situations, but it also provides a mechanism to:"
msgstr ""

#: ../controller-ha-pacemaker.rst:534
msgid ""
"The`` no-quorum-policy=\"ignore\"`` parameter is required in 2-node "
"Pacemaker clusters to disable quorum enforcement. if quorum enforcement is "
"enabled and one of the two nodes fails, then the remaining node can not "
"establish the majority of quorum votes that are necessary to run services. "
"This means that it is unable to take over any resources. Ignoring loss of "
"quorum in the cluster avoids this problem and is appropriate for small "
"configurations used for study or demonstration purposes. Clusters that "
"ignore lose of quorum are vulnerable to split-brain because, if both nodes "
"remain online but lose communication with each other, either node may become "
"active."
msgstr ""

#: ../controller-ha-pacemaker.rst:25
msgid ""
"These agents must conform to one of the `OCF <https://github.com/ClusterLabs/"
"OCF-spec/blob/master/ra/resource-agent-api.md>`_, `SYS-V <http://refspecs."
"linux-foundation.org/LSB_3.0.0/LSB-Core-generic/LSB-Core-generic/iniscrptact."
"html>`_, Upstart, or Systemd standards."
msgstr ""

#: ../storage-ha-cinder.rst:44
msgid ""
"This configuration creates ``p_cinder-api``, a resource for managing the "
"Block Storage API service."
msgstr ""

#: ../storage-ha-glance.rst:48
msgid ""
"This configuration creates ``p_glance-api``, a resource for managing the "
"OpenStack Image API service."
msgstr ""

#: ../install-ha-vip.rst:9
msgid ""
"This configuration creates ``p_ip_api``, a virtual IP address for use by the "
"API node (``192.168.42.103``):"
msgstr ""

#: ../controller-ha-keystone.rst:52
msgid ""
"This configuration creates ``p_keystone``, a resource for managing the "
"OpenStack Identity service."
msgstr ""

#: ../intro-ha-concepts.rst:63
msgid ""
"This document discusses some common methods of implementing highly available "
"systems, with an emphasis on the core OpenStack services and other open "
"source services that are closely aligned with OpenStack. These methods are "
"by no means the only ways to do it; you may supplement these services with "
"commercial hardware and software that provides additional features and "
"functionality. You also need to address high availability concerns for any "
"applications software that you run on your OpenStack environment. The "
"important thing is to make sure that your services are redundant and "
"available; how you achieve that is up to you."
msgstr ""

#: ../controller-ha-galera.rst:298
msgid ""
"This guide assumes that three nodes are used to form the MariaDB Galera "
"cluster. Unless otherwise specified, all commands need to be executed on all "
"cluster nodes."
msgstr ""

#: ../index.rst:8
msgid ""
"This guide describes how to install and configure OpenStack for high "
"availability. It supplements the OpenStack Installation Guides and assumes "
"that you are familiar with the material in those guides."
msgstr ""

#: ../index.rst:13
msgid ""
"This guide documents OpenStack Liberty, OpenStack Kilo, OpenStack Juno, and "
"OpenStack Icehouse releases."
msgstr ""

#: ../index.rst:16
msgid ""
"This guide is a work-in-progress and changing rapidly while we continue to "
"test and enhance the guidance. Please note where there are open \"to do\" "
"items and help where you are able."
msgstr ""

#: ../storage-ha-glance.rst:14
msgid ""
"This section assumes that you are familiar with the `documentation <http://"
"docs.openstack.org/kilo/install-guide/install/apt/content/ch_glance.html>`_ "
"for installing the OpenStack Image API service."
msgstr ""

#: ../storage-ha-backend.rst:44
msgid ""
"This section discusses ways to protect against data loss in your OpenStack "
"environment."
msgstr ""

#: ../controller-ha-keystone.rst:87
msgid ""
"To be sure that all data is highly available, ensure that everything is "
"stored in the MySQL database (which is also highly available):"
msgstr ""

#: ../controller-ha-rabbitmq.rst:106
msgid ""
"To do so, stop RabbitMQ everywhere and copy the cookie from the first node "
"to each of the other node(s):"
msgstr ""

#: ../controller-ha-telemetry.rst:74
msgid ""
"To enable the compute agent to run multiple instances simultaneously with "
"workload partitioning, the workload_partitioning option has to be set to "
"``True`` under the `compute section <http://docs.openstack.org/kilo/ config-"
"reference/content/ch_configuring-openstack-telemetry.html>`__ in the :file:"
"`ceilometer.conf` configuration file."
msgstr ""

#: ../controller-ha-rabbitmq.rst:156
msgid ""
"To ensure that all queues except those with auto-generated names are "
"mirrored across all running nodes, set the `ha-mode` policy key to all by "
"running the following command on one of the nodes:"
msgstr ""

#: ../controller-ha-galera.rst:303
msgid "To install MariaDB with Galera"
msgstr ""

#: ../install-ha-memcached.rst:23
msgid ""
"To install and configure memcached, read the `official documentation "
"<https://code.google.com/p/memcached/wiki/NewStart>`_."
msgstr ""

#: ../controller-ha-rabbitmq.rst:144
msgid "To verify the cluster status:"
msgstr ""

#: ../controller-ha-telemetry.rst:23
msgid ""
"Tooz supports `various drivers <http://docs.openstack.org/developer/tooz/"
"drivers.html>`__ including the following back end solutions:"
msgstr ""

#: ../networking-ha-l3.rst:23 ../networking-ha-l3.rst:26
msgid "True"
msgstr ""

#: ../controller-ha-pacemaker.rst:513
msgid ""
"Type :command:`crm configure` from a shell prompt to jump straight into the "
"Pacemaker configuration menu."
msgstr ""

#: ../intro-ha-concepts.rst:128
msgid ""
"Typically, an active/active installation for a stateless service maintains a "
"redundant instance, and requests are load balanced using a virtual IP "
"address and a load balancer such as HAProxy."
msgstr ""

#: ../controller-ha-rabbitmq.rst:29
msgid "Ubuntu, Debian"
msgstr ""

# #-#-#-#-#  install-ha-os.pot (High Availability Guide 0.0.1)  #-#-#-#-#
# #-#-#-#-#  storage-ha-backend.pot (High Availability Guide 0.0.1)  #-#-#-#-#
#: ../install-ha-os.rst:14 ../storage-ha-backend.rst:40
msgid "Ubuntu: [link to Ubuntu install guide]"
msgstr ""

#: ../install-ha-ntp.rst:13
msgid "Ubuntu: [link to ubuntu install guide]"
msgstr ""

#: ../controller-ha-galera.rst:61
msgid "Update your system and install the required packages:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:214
msgid "Use HA queues in RabbitMQ (x-ha-policy: all):"
msgstr ""

#: ../intro-ha-controller.rst:70
msgid ""
"Use MySQL/Galera in active/passive mode to avoid deadlocks on ``SELECT ... "
"FOR UPDATE`` type queries (used, for example, by nova and neutron). This "
"issue is discussed more in the following:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:208
msgid "Use durable queues in RabbitMQ:"
msgstr ""

#: ../controller-ha-pacemaker.rst:426
msgid ""
"Use the :command:`corosync-cfgtool` utility with the -s option to get a "
"summary of the health of the communication rings:"
msgstr ""

#: ../controller-ha-pacemaker.rst:441
msgid ""
"Use the :command:`corosync-objctl` utility to dump the Corosync cluster "
"member list:"
msgstr ""

#: ../controller-ha-galera.rst:169
msgid ""
"Use the following command after the copy to verify that all files are "
"identical:"
msgstr ""

#: ../networking-ha-l3.rst:20
msgid "Value"
msgstr ""

#: ../controller-ha-galera.rst:126
msgid ""
"Verify that the nodes can access each other through the firewall. On Red "
"Hat, this means adjusting :manpage:`iptables(8)`, as in:"
msgstr ""

#: ../controller-ha-galera.rst:228
msgid ""
"Verify the wsrep replication by logging in as root under mysql and running "
"the following command:"
msgstr ""

#: ../controller-ha-rabbitmq.rst:71
msgid ""
"We are building a cluster of RabbitMQ nodes to construct a RabbitMQ broker, "
"which is a logical grouping of several Erlang nodes."
msgstr ""

#: ../controller-ha-rabbitmq.rst:176
msgid ""
"We have to configure the OpenStack components to use at least two RabbitMQ "
"nodes."
msgstr ""

#: ../controller-ha-rabbitmq.rst:87
msgid ""
"We have to consider that, while exchanges and bindings survive the loss of "
"individual nodes, queues and their messages do not because a queue and its "
"contents are located on one node. If we lose this node, we also lose the "
"queue."
msgstr ""

#: ../storage-ha-backend.rst:77
msgid ""
"When Ceph RBD is used for ephemeral volumes as well as block and image "
"storage, it supports `live migration <http://docs.openstack.org/admin-guide-"
"cloud/compute-live-migration-usage.html>`_ of VMs with ephemeral drives; LVM "
"only supports live migration of volume-backed VMs."
msgstr ""

#: ../intro-ha-concepts.rst:162
msgid ""
"When configuring an OpenStack environment for study or demonstration "
"purposes, it is possible to turn off the quorum checking; this is discussed "
"later in this guide. Production systems should always run with quorum "
"enabled."
msgstr ""

#: ../hardware-ha-basic.rst:46
msgid ""
"When installing highly-available OpenStack on VMs, be sure that your "
"hypervisor permits promiscuous mode and disables MAC address filtering on "
"the external network."
msgstr ""

#: ../controller-ha-pacemaker.rst:187
msgid ""
"With ``secauth`` enabled, Corosync nodes mutually authenticate using a 128-"
"byte shared secret stored in the :file:`/etc/corosync/authkey` file, which "
"may be generated with the :command:`corosync-keygen` utility. When using "
"``secauth``, cluster communications are also encrypted."
msgstr ""

#: ../controller-ha-pacemaker.rst:297
msgid ""
"Within the ``nodelist`` directive, it is possible to specify specific "
"information about the nodes in the cluster. The directive can contain only "
"the node sub-directive, which specifies every node that should be a member "
"of the membership, and where non-default options are needed. Every node must "
"have at least the ``ring0_addr`` field filled."
msgstr ""

#: ../controller-ha-telemetry.rst:46
msgid ""
"Without the ``backend_url`` option being set only one instance of both the "
"central and compute agent service is able to run and function correctly."
msgstr ""

#: ../controller-ha-keystone.rst:121
msgid ""
"You also need to create the OpenStack Identity Endpoint with this IP address."
msgstr ""

#: ../controller-ha-galera.rst:26
msgid ""
"You can also use PostgreSQL, which has its own replication, or another "
"database HA option."
msgstr ""

#: ../controller-ha-galera.rst:58
msgid ""
"You can choose a different mirror from the list at `downloads.mariadb.org "
"<https://downloads.mariadb.org>`_"
msgstr ""

#: ../controller-ha-galera.rst:39
msgid ""
"You can find additional information about installing and configuring Galera/"
"MySQL in:"
msgstr ""

#: ../storage-ha-cinder.rst:28
msgid ""
"You can now add the Pacemaker configuration for Block Storage API resource. "
"Connect to the Pacemaker cluster with the :command:`crm configure` command "
"and add the following cluster resources:"
msgstr ""

#: ../storage-ha-glance.rst:33
msgid ""
"You can now add the Pacemaker configuration for the OpenStack Image API "
"resource. Use the :command:`crm configure` command to connect to the "
"Pacemaker cluster and add the following cluster resources:"
msgstr ""

#: ../controller-ha-keystone.rst:36
msgid ""
"You can now add the Pacemaker configuratioon for the OpenStack Identity "
"resource by running the :command:`crm configure` command to connect to the "
"Pacemaker cluster. Add the following cluster resources:"
msgstr ""

#: ../controller-ha-pacemaker.rst:424
msgid "You can now check the Corosync connectivity with two tools."
msgstr ""

#: ../controller-ha-galera.rst:139
msgid ""
"You may also need to configure any NAT firewall between nodes to allow "
"direct connections. You may need to disable SELinux or configure it to "
"allow :command:`mysqld` to listen to sockets at unprivileged ports. See the "
"`Firewalls and default ports <http://docs.openstack.org/kilo/config-"
"reference/content/firewalls-default-ports.html>`_ section of the "
"Configuration Reference."
msgstr ""

#: ../storage-ha-glance.rst:124
msgid ""
"You must also create the OpenStack Image API endpoint with this IP address. "
"If you are using both private and public IP addresses, you should create two "
"Virtual IP addresses and define your endpoint like this:"
msgstr ""

#: ../controller-ha-telemetry.rst:36
msgid ""
"You must configure a supported Tooz driver for the HA deployment of the "
"Telemetry services."
msgstr ""

#: ../storage-ha-cinder.rst:96
msgid "You must create the Block Storage API endpoint with this IP."
msgstr ""

#: ../controller-ha-keystone.rst:25
msgid ""
"You must first download the OpenStack Identity resource to Pacemaker by "
"running the following commands:"
msgstr ""

# #-#-#-#-#  storage-ha-cinder.pot (High Availability Guide 0.0.1)  #-#-#-#-#
# #-#-#-#-#  storage-ha-glance.pot (High Availability Guide 0.0.1)  #-#-#-#-#
#: ../storage-ha-cinder.rst:20 ../storage-ha-glance.rst:24
msgid "You must first download the resource agent to your system:"
msgstr ""

#: ../install-ha-ntp.rst:6
msgid ""
"You must install NTP to properly synchronize services among nodes. We "
"recommend that you configure the controller node to reference more accurate "
"(lower stratum) servers and other nodes to reference the controller node."
msgstr ""

#: ../install-ha-vip.rst:6
msgid ""
"You must select and assign a virtual IP address (VIP) that can freely float "
"between cluster nodes."
msgstr ""

#: ../controller-ha-galera.rst:177
msgid ""
"You need to get the database password from the :file:`debian.cnf` file. You "
"can do this with the following command:"
msgstr ""

#: ../controller-ha-pacemaker.rst:454
msgid ""
"You should see a ``status=joined`` entry for each of your constituent "
"cluster nodes."
msgstr ""

#: ../storage-ha-cinder.rst:90
msgid ""
"Your OpenStack services must now point their Block Storage API configuration "
"to the highly available, virtual cluster IP address rather than a Block "
"Storage API server’s physical IP address as you would for a non-HA "
"environment."
msgstr ""

#: ../controller-ha-keystone.rst:106
msgid ""
"Your OpenStack services must now point their OpenStack Identity "
"configuration to the highly available virtual cluster IP address rather than "
"point to the physical IP address of an OpenStack Identity server as you "
"would do in a non-HA environment."
msgstr ""

#: ../storage-ha-glance.rst:99
msgid ""
"Your OpenStack services must now point their OpenStack Image API "
"configuration to the highly available, virtual cluster IP address instead of "
"pointint to the physical IP address of an OpenStack Image API server as you "
"would in a non-HA cluster."
msgstr ""

#: ../controller-ha-telemetry.rst:6
msgid "[TODO (Add Telemetry overview)]"
msgstr ""

#: ../controller-ha-haproxy.rst:13
msgid "[TODO (Add note about using commercial load-balancers]"
msgstr ""

#: ../install-ha.rst:5
msgid "[TODO -- write intro to this section]"
msgstr ""

#: ../intro-ha-controller.rst:9
msgid ""
"[TODO Discuss SLA (Service Level Agreement), if this is the measure we use. "
"Other possibilities include MTTR (Mean Time To Recover), RTO (Recovery Time "
"Objective), and ETR (Expected Time of Repair,]"
msgstr ""

#: ../intro-ha-controller.rst:24
msgid ""
"[TODO Need description of VIP failover inside Linux namespaces and expected "
"SLA.]"
msgstr ""

#: ../intro-ha-controller.rst:17
msgid ""
"[TODO Need discussion of network hardware, bonding interfaces, intelligent "
"Layer 2 switches, routers and Layer 3 switches.]"
msgstr ""

#: ../install-ha-os.rst:10
msgid "[TODO(DreidelLhasa): Replace the following with real links]"
msgstr ""

#: ../install-ha-memcached.rst:6
msgid ""
"[TODO:  Verify that Oslo supports hash synchronization; if so, this should "
"not take more than load balancing.]"
msgstr ""

#: ../storage-ha-backend.rst:87
msgid ""
"[TODO: Add discussion of remote backup facilities as an alternate way to "
"secure ones data. Include brief mention of key third-party technologies with "
"links to their documentation]"
msgstr ""

#: ../controller-ha-rabbitmq.rst:78
msgid ""
"[TODO: Does this list need to be updated?  Perhaps we need a table that "
"shows each component and the earliest release that allows it to work with HA "
"queues.]"
msgstr ""

#: ../hardware-ha.rst:6
msgid ""
"[TODO: Provide a minimal architecture example for HA, expanded on that given "
"in http://docs.openstack.org/juno/install-guide/install/apt/content/"
"ch_basic_environment.html#basics-prerequisites for easy comparison]"
msgstr ""

#: ../controller-ha-rabbitmq.rst:104
msgid "[TODO: Should the example instead use a minimum of three nodes?]"
msgstr ""

#: ../controller-ha-pacemaker.rst:457
msgid ""
"[TODO: Should the main example now use corosync-cmapctl and have the note "
"give the command for Corosync version 1?]"
msgstr ""

#: ../controller-ha-galera.rst:315
msgid "[TODO: Should this be moved to some other place?]"
msgstr ""

#: ../install-ha-memcached.rst:30
msgid "[TODO: Should this show three hosts?]"
msgstr ""

#: ../install-ha-memcached.rst:9
msgid ""
"[TODO: This hands off to two different docs for install information. We "
"should choose one or explain the specific purpose of each.]"
msgstr ""

#: ../controller-ha-rabbitmq.rst:68
msgid ""
"[TODO: This section should begin with a brief mention about what HA queues "
"are and why they are valuable, etc]"
msgstr ""

#: ../networking-ha-metadata.rst:14
msgid ""
"[TODO: Update this information. Can this service now be made HA in active/"
"active mode or do we need to pull in the instructions to run this service in "
"active/passive mode?]"
msgstr ""

#: ../networking-ha.rst:23
msgid ""
"[TODO: Verify that the active/passive network configuration information from "
"`<http://docs.openstack.org/high-availability-guide/content/s-neutron-server."
"html>`_ should not be included here."
msgstr ""

#: ../hardware-ha-basic.rst:16
msgid "[TODO: Verify that these numbers are good]"
msgstr ""

#: ../storage-ha-backend.rst:37
msgid "[TODO: add proper links to the install guides]"
msgstr ""

#: ../controller-ha-galera.rst:441
msgid "[TODO: is the kill command necessary here?]"
msgstr ""

#: ../storage-ha-glance.rst:92
msgid "[TODO: need more discussion of these parameters]"
msgstr ""

#: ../controller-ha-galera.rst:48
msgid "[TODO: provide instructions for SUSE and Red Hat]"
msgstr ""

#: ../controller-ha-rabbitmq.rst:76
msgid "[TODO: replace \"currently\" with specific release names]"
msgstr ""

#: ../controller-ha-galera.rst:29
msgid ""
"[TODO: the structure of the MySQL and MariaDB sections should be made "
"parallel]"
msgstr ""

#: ../networking-ha-lbaas.rst:16
msgid "[TODO: update this section.]"
msgstr ""

#: ../controller-ha-haproxy.rst:198
msgid ""
"[TODO: we need more commentary about the contents and format of this file]"
msgstr ""

#: ../controller-ha-rabbitmq.rst:38
msgid "[Verify fingerprint of imported GPG key; see below]"
msgstr ""

#: ../storage-ha-backend.rst:58
msgid ""
"`Ceph RBD <http://ceph.com/>`_ is an innately high availability storage back "
"end. It creates a storage cluster with multiple nodes that communicate with "
"each other to replicate and redistribute data dynamically. A Ceph RBD "
"storage cluster provides a single shared set of storage nodes that can "
"handle all classes of persistent and ephemeral data -- glance, cinder, and "
"nova -- that are required for OpenStack instances."
msgstr ""

#: ../controller-ha-rabbitmq.rst:168
msgid "`Clustering Guide <https://www.rabbitmq.com/clustering.html>`_"
msgstr ""

#: ../controller-ha-rabbitmq.rst:59
msgid "`Debian and Ubuntu <http://www.rabbitmq.com/install-debian.html>`_"
msgstr ""

#: ../controller-ha-galera.rst:43
msgid ""
"`Galera Getting Started guide <http://galeracluster.com/documentation-"
"webpages/gettingstarted.html>`_"
msgstr ""

#: ../controller-ha-rabbitmq.rst:167
msgid "`Highly Available Queues <http://www.rabbitmq.com/ha.html>`_"
msgstr ""

#: ../networking-ha.rst:28
msgid ""
"`LP1328922 <https://bugs.launchpad.net/openstack-manuals/+bug/1328922>` and "
"`LP1349398 <https://bugs.launchpad.net/openstack-manuals/+bug/1349398>` are "
"related.]"
msgstr ""

#: ../controller-ha-galera.rst:21
msgid ""
"`MariaDB Galera Cluster <https://mariadb.org/>`_ is supported for "
"environments based on Red Hat distributions; configuration instructions are "
"in :ref:`maria-db-ha`."
msgstr ""

#: ../controller-ha-telemetry.rst:34
msgid "`Memcached <http://memcached.org/>`__."
msgstr ""

#: ../controller-ha-galera.rst:24
msgid "`Percona XtraDB Cluster <http://www.percona.com/>`_ works with Galera."
msgstr ""

#: ../controller-ha-rabbitmq.rst:60
msgid ""
"`RPM based <http://www.rabbitmq.com/install-rpm.html>`_ (RHEL, Fedora, "
"CentOS, openSUSE)"
msgstr ""

#: ../controller-ha-telemetry.rst:31
msgid "`Redis <http://redis.io/>`__."
msgstr ""

#: ../controller-ha-telemetry.rst:28
msgid "`Zookeeper <http://zookeeper.apache.org/>`__."
msgstr ""

#: ../controller-ha-pacemaker.rst:404
msgid "``last_man_standing_window`` specifies the time, in milliseconds,"
msgstr ""

#: ../controller-ha-pacemaker.rst:315
msgid ""
"``nodeid`` is optional when using IPv4 and required when using IPv6. This is "
"a 32-bit value specifying the node identifier delivered to the cluster "
"membership service. If this is not specified with IPv4, the node id is "
"determined from the 32-bit IP address of the system to which the system is "
"bound with ring identifier of 0. The node identifier value of zero is "
"reserved and should not be used."
msgstr ""

#: ../controller-ha-pacemaker.rst:312
msgid ""
"``ring{X}_addr`` specifies the IP address of one of the nodes. {X} is the "
"ring number."
msgstr ""

#: ../intro-ha-controller.rst:75
msgid ""
"`http://lists.openstack.org/pipermail/openstack-dev/2014-May/035264.html`"
msgstr ""

#: ../intro-ha-controller.rst:76 ../intro-ha-controller.rst:77
msgid "`http://www.joinfu.com/`"
msgstr ""

#: ../hardware-ha-basic.rst:8
msgid ""
"`neutron <http://docs.openstack.org/juno/install-guide/install/apt/content/"
"ch_overview.html#example-architecture-with-neutron-networking-hw>`_"
msgstr ""

#: ../hardware-ha-basic.rst:9
msgid ""
"`nova-network <http://docs.openstack.org/juno/install-guide/install/apt/"
"content/ch_overview.html#example-architecture-with-legacy-networking-hw>`_"
msgstr ""

#: ../controller-ha-galera.rst:42
msgid ""
"`wsrep readme file <https://launchpadlibrarian.net/66669857/README-wsrep>`_"
msgstr ""

#: ../networking-ha-l3.rst:25
msgid "allow_automatic_l3agent_failover"
msgstr ""

#: ../controller-ha-pacemaker.rst:64
msgid "cluster-glue"
msgstr ""

#: ../hardware-ha-basic.rst:25
msgid "compute node"
msgstr ""

#: ../hardware-ha-basic.rst:21
msgid "controller node"
msgstr ""

#: ../controller-ha-pacemaker.rst:62
msgid "corosync"
msgstr ""

#: ../controller-ha-pacemaker.rst:60
msgid "crmsh"
msgstr ""

#: ../controller-ha-pacemaker.rst:66
msgid ""
"fence-agents (Fedora only; all other distributions use fencing agents from "
"cluster-glue)"
msgstr ""

#: ../networking-ha-l3.rst:22
msgid "l3_ha"
msgstr ""

#: ../networking-ha-l3.rst:28
msgid "max_l3_agents_per_router"
msgstr ""

#: ../networking-ha-l3.rst:31
msgid "min_l3_agents_per_router"
msgstr ""

#: ../hardware-ha-basic.rst:23
msgid "network node"
msgstr ""

#: ../controller-ha-rabbitmq.rst:33
msgid "openSUSE"
msgstr ""

#: ../storage-ha-backend.rst:39
msgid "openSUSE and SUSE Linux Enterprise: [link to SUSE install guide]"
msgstr ""

#: ../install-ha-os.rst:13
msgid "openSUSE, SUSE Linux Enterprise Server: [link to SUSE install guide]"
msgstr ""

#: ../controller-ha-pacemaker.rst:58
msgid "pacemaker (Note that the crm shell should be downloaded separately.)"
msgstr ""

#: ../controller-ha-pacemaker.rst:398
msgid ""
"required to recalculate quorum after one or most hosts have been lost from "
"the cluster. To do the new quorum recalculation, the cluster must have "
"quorum for at least the interval specified for  "
"``last_man_standing_window``; the default is 10000ms."
msgstr ""

#: ../controller-ha-pacemaker.rst:69
msgid "resource-agents"
msgstr ""
